@inproceedings{Vinh:2009:mutual,
	author = {Vinh, Nguyen Xuan and Epps, Julien and Bailey, James},
	title = {Information Theoretic Measures for Clusterings Comparison: Is a Correction for Chance Necessary?},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	series = {ICML '09},
	year = {2009},
	isbn = {978-1-60558-516-1},
	location = {Montreal, Quebec, Canada},
	pages = {1073--1080},
	numpages = {8},
	url = {http://doi.acm.org/10.1145/1553374.1553511},
	doi = {10.1145/1553374.1553511},
	acmid = {1553511},
	publisher = {ACM},
	address = {New York, NY, USA},
} 
@article{hubert_comparing_1985,
	title = {Comparing partitions},
	volume = {2},
	issn = {1432-1343},
	url = {https://doi.org/10.1007/BF01908075},
	doi = {10.1007/BF01908075},
	abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by reviewing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Miligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from corresponding partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addition to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between ±1.},
	number = {1},
	journal = {Journal of Classification},
	author = {Hubert, Lawrence and Arabie, Phipps},
	month = dec,
	year = {1985},
	pages = {193--218}
}

@inproceedings{grira_active_2005,
	address = {Hilton, Singapore},
	title = {Active semi-supervised fuzzy clustering for image database categorization},
	isbn = {978-1-59593-244-0},
	url = {http://portal.acm.org/citation.cfm?doid=1101826.1101831},
	doi = {10.1145/1101826.1101831},
	abstract = {We consider data clustering problems where a limited amount of high-level semantic information can be generated. This form of supervision will guide the categorization of image databases in order to provide good overviews and make their access more eﬀective. Traditional algorithms usually rely on a pre-deﬁned similarity measure between unlabelled data to attempt to identify natural classes of items. When compared to what a human expert would provide on the same data, the results obtained may be disappointing if the similarity measure employed by the system is too diﬀerent from the one a human would use. To obtain clusters ﬁtting user expectations better, we can exploit, in addition to the unlabelled data, some limited form of supervision, such as constraints specifying whether two data items belong to a same cluster or not. The resulting approach is called semi-supervised clustering. We propose here an effective semi-supervised clustering algorithm, Active Fuzzy Constrained Clustering (AFCC), that minimizes a competitive agglomeration-based cost function with fuzzy terms corresponding to pairwise constraints provided by the user. In order to minimize the amount of constraints required, we deﬁne an active mechanism for the selection of candidates for constraints. The comparisons performed on a simple benchmark and on a ground truth image database show that with AFCC the results of clustering can be signiﬁcantly improved with few constraints, making this semi-supervised approach an attractive alternative in the categorization of image databases.},
	language = {en},
	urldate = {2018-12-14},
	booktitle = {Proceedings of the 7th {ACM} {SIGMM} international workshop on {Multimedia} information retrieval  - {MIR} '05},
	publisher = {ACM Press},
	author = {Grira, Nizar and Crucianu, Michel and Boujemaa, Nozha},
	year = {2005},
	pages = {9},
	file = {Grira et al. - 2005 - Active semi-supervised fuzzy clustering for image .pdf:C\:\\Users\\yujiad2\\Zotero\\storage\\YQQRJVSC\\Grira et al. - 2005 - Active semi-supervised fuzzy clustering for image .pdf:application/pdf}
}
@inproceedings{dasgupta_hierarchical_2008,
	address = {Helsinki, Finland},
	title = {Hierarchical sampling for active learning},
	isbn = {978-1-60558-205-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390183},
	doi = {10.1145/1390156.1390183},
	abstract = {We present an active learning scheme that exploits cluster structure in data.},
	language = {en},
	urldate = {2018-12-14},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning - {ICML} '08},
	publisher = {ACM Press},
	author = {Dasgupta, Sanjoy and Hsu, Daniel},
	year = {2008},
	pages = {208--215},
	file = {Dasgupta and Hsu - 2008 - Hierarchical sampling for active learning.pdf:C\:\\Users\\yujiad2\\Zotero\\storage\\52BPLZUT\\Dasgupta and Hsu - 2008 - Hierarchical sampling for active learning.pdf:application/pdf}
}

@inproceedings{hofmann_active_1998,
	title = {Active data clustering},
	pages = {528--534},
	booktitle = {Advances in Neural Information Processing Systems},
	author = {Hofmann, Thomas and Buhmann, Joachim M.},
	year = {1998},
	file = {Full Text:C\:\\Users\\yujiad2\\Zotero\\storage\\CVRJJFRI\\Hofmann and Buhmann - 1998 - Active data clustering.pdf:application/pdf}
}

@inproceedings{druck_active_2009,
	location = {Singapore},
	title = {Active learning by labeling features},
	volume = {1},
	isbn = {978-1-932432-59-6},
	url = {http://portal.acm.org/citation.cfm?doid=1699510.1699522},
	doi = {10.3115/1699510.1699522},
	abstract = {Methods that learn from prior information about input features such as generalized expectation ({GE}) have been used to train accurate models with very little effort. In this paper, we propose an active learning approach in which the machine solicits “labels” on features rather than instances. In both simulated and real user experiments on two sequence labeling tasks we show that our active learning method outperforms passive learning with features as well as traditional active learning with instances. Preliminary experiments suggest that novel interfaces which intelligently solicit labels on multiple features facilitate more efﬁcient annotation.},
	eventtitle = {the 2009 Conference},
	pages = {81},
	booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing Volume 1 - {EMNLP} '09},
	publisher = {Association for Computational Linguistics},
	author = {Druck, Gregory and Settles, Burr and {McCallum}, Andrew},
	urldate = {2018-12-14},
	year = {2009},
	langid = {english},
	file = {Druck et al. - 2009 - Active learning by labeling features.pdf:C\:\\Users\\yujiad2\\Zotero\\storage\\Z68RCWN6\\Druck et al. - 2009 - Active learning by labeling features.pdf:application/pdf}
}



@inproceedings{andrzejewski_incorporating_2009,
	address = {Montreal, Quebec, Canada},
	title = {Incorporating domain knowledge into topic modeling via {Dirichlet} {Forest} priors},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553378},
	doi = {10.1145/1553374.1553378},
	abstract = {Users of topic modeling methods often have knowledge about the composition of words that should have high or low probability in various topics. We incorporate such domain knowledge using a novel Dirichlet Forest prior in a Latent Dirichlet Allocation framework. The prior is a mixture of Dirichlet tree distributions with special structures. We present its construction, and inference via collapsed Gibbs sampling. Experiments on synthetic and real datasets demonstrate our model’s ability to follow and generalize beyond userspeciﬁed domain knowledge.},
	language = {en},
	urldate = {2018-12-14},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Andrzejewski, David and Zhu, Xiaojin and Craven, Mark},
	year = {2009},
	pages = {1--8},
	file = {Andrzejewski et al. - 2009 - Incorporating domain knowledge into topic modeling.pdf:C\:\\Users\\yujiad2\\Zotero\\storage\\E6VMRTHP\\Andrzejewski et al. - 2009 - Incorporating domain knowledge into topic modeling.pdf:application/pdf}
}

@inproceedings{druck_learning_2008,
	address = {Singapore, Singapore},
	title = {Learning from labeled features using generalized expectation criteria},
	isbn = {978-1-60558-164-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390334.1390436},
	doi = {10.1145/1390334.1390436},
	language = {en},
	urldate = {2018-12-14},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '08},
	publisher = {ACM Press},
	author = {Druck, Gregory and Mann, Gideon and McCallum, Andrew},
	year = {2008},
	pages = {595},
	file = {Druck et al. - 2008 - Learning from labeled features using generalized e.pdf:C\:\\Users\\yujiad2\\Zotero\\storage\\QR9XIKKJ\\Druck et al. - 2008 - Learning from labeled features using generalized e.pdf:application/pdf}
}

@inproceedings{liang_learning_2009,
	address = {Montreal, Quebec, Canada},
	title = {Learning from measurements in exponential families},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553457},
	doi = {10.1145/1553374.1553457},
	abstract = {Given a model family and a set of unlabeled examples, one could either label speciﬁc examples or state general constraints—both provide information about the desired model. In general, what is the most cost-eﬀective way to learn? To address this question, we introduce measurements, a general class of mechanisms for providing information about a target model. We present a Bayesian decision-theoretic framework, which allows us to both integrate diverse measurements and choose new measurements to make. We use a variational inference algorithm, which exploits exponential family duality. The merits of our approach are demonstrated on two sequence labeling tasks.},
	language = {en},
	urldate = {2018-12-14},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Liang, Percy and Jordan, Michael I. and Klein, Dan},
	year = {2009},
	pages = {1--8},
	file = {Liang et al. - 2009 - Learning from measurements in exponential families.pdf:C\:\\Users\\yujiad2\\Zotero\\storage\\F9K6Y32D\\Liang et al. - 2009 - Learning from measurements in exponential families.pdf:application/pdf}
}

@inproceedings{haghighi_prototype-driven_2006,
	address = {New York, New York},
	title = {Prototype-driven learning for sequence models},
	url = {http://portal.acm.org/citation.cfm?doid=1220835.1220876},
	doi = {10.3115/1220835.1220876},
	abstract = {We investigate prototype-driven learning for primarily unsupervised sequence modeling. Prior knowledge is speciﬁed declaratively, by providing a few canonical examples of each target annotation label. This sparse prototype information is then propagated across a corpus using distributional similarity features in a log-linear generative model. On part-of-speech induction in English and Chinese, as well as an information extraction task, prototype features provide substantial error rate reductions over competitive baselines and outperform previous work. For example, we can achieve an English part-of-speech tagging accuracy of 80.5\% using only three examples of each tag and no dictionary constraints. We also compare to semi-supervised learning and discuss the system’s error trends.},
	language = {en},
	urldate = {2018-12-14},
	booktitle = {Proceedings of the main conference on {Human} {Language} {Technology} {Conference} of the {North} {American} {Chapter} of the {Association} of {Computational} {Linguistics}  -},
	publisher = {Association for Computational Linguistics},
	author = {Haghighi, Aria and Klein, Dan},
	year = {2006},
	pages = {320--327},
	file = {Haghighi and Klein - 2006 - Prototype-driven learning for sequence models.pdf:C\:\\Users\\yujiad2\\Zotero\\storage\\C5YMNCQT\\Haghighi and Klein - 2006 - Prototype-driven learning for sequence models.pdf:application/pdf}
}

@inproceedings{huang_text_2006,
	address = {Seattle, Washington, USA},
	title = {Text clustering with extended user feedback},
	isbn = {978-1-59593-369-0},
	url = {http://portal.acm.org/citation.cfm?doid=1148170.1148242},
	doi = {10.1145/1148170.1148242},
	abstract = {Text clustering is most commonly treated as a fully automated task without user feedback. However, a variety of researchers have explored mixed-initiative clustering methods which allow a user to interact with and advise the clustering algorithm. This mixed-initiative approach is especially attractive for text clustering tasks where the user is trying to organize a corpus of documents into clusters for some particular purpose (e.g., clustering their email into folders that reﬂect various activities in which they are involved). This paper introduces a new approach to mixed-initiative clustering that handles several natural types of user feedback. We ﬁrst introduce a new probabilistic generative model for text clustering (the SpeClustering model) and show that it outperforms the commonly used mixture of multinomials clustering model, even when used in fully autonomous mode with no user input. We then describe how to incorporate four distinct types of user feedback into the clustering algorithm, and provide experimental evidence showing substantial improvements in text clustering when this user feedback is incorporated.},
	language = {en},
	urldate = {2018-12-14},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval  - {SIGIR} '06},
	publisher = {ACM Press},
	author = {Huang, Yifen and Mitchell, Tom M.},
	year = {2006},
	pages = {413},
	file = {Huang and Mitchell - 2006 - Text clustering with extended user feedback.pdf:C\:\\Users\\yujiad2\\Zotero\\storage\\784NXQIU\\Huang and Mitchell - 2006 - Text clustering with extended user feedback.pdf:application/pdf}
}

@inproceedings{sindhwani_uncertainty_2009,
	address = {Montreal, Quebec, Canada},
	title = {Uncertainty sampling and transductive experimental design for active dual supervision},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553496},
	doi = {10.1145/1553374.1553496},
	abstract = {Dual supervision refers to the general setting of learning from both labeled examples as well as labeled features. Labeled features are naturally available in tasks such as text classiﬁcation where it is frequently possible to provide domain knowledge in the form of words that associate strongly with a class. In this paper, we consider the novel problem of active dual supervision, or, how to optimally query an example and feature labeling oracle to simultaneously collect two diﬀerent forms of supervision, with the objective of building the best classiﬁer in the most cost eﬀective manner. We apply classical uncertainty and experimental design based active learning schemes to graph/kernel-based dual supervision models. Empirical studies conﬁrm the potential of these schemes to signiﬁcantly reduce the cost of acquiring labeled data for training high-quality models.},
	language = {en},
	urldate = {2018-12-14},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Sindhwani, Vikas and Melville, Prem and Lawrence, Richard D.},
	year = {2009},
	pages = {1--8},
	file = {Sindhwani et al. - 2009 - Uncertainty sampling and transductive experimental.pdf:C\:\\Users\\yujiad2\\Zotero\\storage\\LZ47WAMF\\Sindhwani et al. - 2009 - Uncertainty sampling and transductive experimental.pdf:application/pdf}
	
@article{raghavan2006active,
  title={Active learning with feedback on features and instances},
  author={Raghavan, Hema and Madani, Omid and Jones, Rosie},
  journal={Journal of Machine Learning Research},
  volume={7},
  number={Aug},
  pages={1655--1686},
  year={2006}
}
	
}